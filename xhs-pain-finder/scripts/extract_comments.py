#!/usr/bin/env python3
"""
å°çº¢ä¹¦è¯„è®ºæŠ“å–è„šæœ¬ - æµè§ˆå™¨è¾…åŠ©æ¨¡å¼
ä½¿ç”¨ Playwright è‡ªåŠ¨åŒ–æµè§ˆå™¨æŠ“å–è¯„è®ºæ•°æ®
"""

import argparse
import json
import time
import re
from datetime import datetime
from pathlib import Path

def extract_comments(url: str, output_path: str, max_scroll: int = 50, headless: bool = False):
    """
    ä»Žå°çº¢ä¹¦å¸–å­æŠ“å–è¯„è®º
    
    Args:
        url: å°çº¢ä¹¦å¸–å­é“¾æŽ¥
        output_path: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„
        max_scroll: æœ€å¤§æ»šåŠ¨æ¬¡æ•°
        headless: æ˜¯å¦æ— å¤´æ¨¡å¼
    """
    try:
        from playwright.sync_api import sync_playwright
    except ImportError:
        print("é”™è¯¯: è¯·å…ˆå®‰è£… playwright: pip install playwright && playwright install chromium")
        return None
    
    comments = []
    
    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ - ä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•ä¿æŒç™»å½•çŠ¶æ€
        user_data_dir = Path.home() / ".xhs-browser-data"
        user_data_dir.mkdir(exist_ok=True)
        
        context = p.chromium.launch_persistent_context(
            user_data_dir=str(user_data_dir),
            headless=headless,
            viewport={"width": 1280, "height": 800},
            locale="zh-CN"
        )
        
        page = context.pages[0] if context.pages else context.new_page()
        
        print(f"æ­£åœ¨è®¿é—®: {url}")
        page.goto(url, wait_until="networkidle", timeout=60000)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(3)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
        if "login" in page.url.lower():
            print("\nâš ï¸ éœ€è¦ç™»å½•ï¼è¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­ç™»å½•å°çº¢ä¹¦è´¦å·...")
            print("ç™»å½•å®ŒæˆåŽï¼Œè„šæœ¬å°†è‡ªåŠ¨ç»§ç»­ã€‚")
            page.wait_for_url(lambda u: "login" not in u.lower(), timeout=300000)
            page.goto(url, wait_until="networkidle")
            time.sleep(3)
        
        # èŽ·å–å¸–å­æ ‡é¢˜
        title = ""
        try:
            title_elem = page.query_selector(".title, .note-title, h1")
            if title_elem:
                title = title_elem.inner_text().strip()
        except:
            pass
        
        print(f"å¸–å­æ ‡é¢˜: {title}")
        print("å¼€å§‹æŠ“å–è¯„è®º...")
        
        # ç‚¹å‡»å±•å¼€è¯„è®ºåŒºï¼ˆå¦‚æžœéœ€è¦ï¼‰
        try:
            expand_btn = page.query_selector('[class*="comment"] button, .show-more')
            if expand_btn:
                expand_btn.click()
                time.sleep(1)
        except:
            pass
        
        # æ»šåŠ¨åŠ è½½æ›´å¤šè¯„è®º
        last_count = 0
        no_new_count = 0
        
        for i in range(max_scroll):
            # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(1.5)
            
            # å°è¯•ç‚¹å‡»"æŸ¥çœ‹æ›´å¤šè¯„è®º"æŒ‰é’®
            try:
                more_btn = page.query_selector('[class*="more"], .load-more, [class*="å±•å¼€"]')
                if more_btn and more_btn.is_visible():
                    more_btn.click()
                    time.sleep(1)
            except:
                pass
            
            # èŽ·å–å½“å‰è¯„è®ºæ•°é‡
            comment_elems = page.query_selector_all('[class*="comment-item"], [class*="commentItem"], .comment')
            current_count = len(comment_elems)
            
            print(f"  æ»šåŠ¨ {i+1}/{max_scroll}, å·²å‘çŽ° {current_count} æ¡è¯„è®º")
            
            if current_count == last_count:
                no_new_count += 1
                if no_new_count >= 5:
                    print("  è¿žç»­5æ¬¡æ— æ–°è¯„è®ºï¼Œåœæ­¢æ»šåŠ¨")
                    break
            else:
                no_new_count = 0
            
            last_count = current_count
        
        # æå–è¯„è®ºæ•°æ®
        print("\næ­£åœ¨æå–è¯„è®ºæ•°æ®...")
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = [
            '[class*="comment-item"]',
            '[class*="commentItem"]',
            '.comment-inner',
            '[class*="comment"] > div'
        ]
        
        comment_elems = []
        for selector in selectors:
            comment_elems = page.query_selector_all(selector)
            if comment_elems:
                break
        
        for idx, elem in enumerate(comment_elems):
            try:
                comment_data = extract_single_comment(elem, idx + 1)
                if comment_data and comment_data.get("content"):
                    comments.append(comment_data)
            except Exception as e:
                print(f"  æå–è¯„è®º {idx+1} å¤±è´¥: {e}")
        
        context.close()
    
    # ä¿å­˜ç»“æžœ
    result = {
        "url": url,
        "title": title,
        "crawl_time": datetime.now().isoformat(),
        "total_comments": len(comments),
        "comments": comments
    }
    
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æŠ“å–å®Œæˆï¼å…± {len(comments)} æ¡è¯„è®º")
    print(f"ðŸ“„ ä¿å­˜è‡³: {output_file}")
    
    return result


def extract_single_comment(elem, index: int) -> dict:
    """æå–å•æ¡è¯„è®ºæ•°æ®"""
    
    # è¯„è®ºå†…å®¹
    content = ""
    content_selectors = ['[class*="content"]', '.text', 'p', 'span']
    for sel in content_selectors:
        try:
            content_elem = elem.query_selector(sel)
            if content_elem:
                content = content_elem.inner_text().strip()
                if content and len(content) > 2:
                    break
        except:
            pass
    
    # ç”¨æˆ·æ˜µç§°
    nickname = ""
    nick_selectors = ['[class*="nickname"]', '[class*="name"]', '.user', 'a']
    for sel in nick_selectors:
        try:
            nick_elem = elem.query_selector(sel)
            if nick_elem:
                nickname = nick_elem.inner_text().strip()
                if nickname and len(nickname) < 30:
                    break
        except:
            pass
    
    # ç‚¹èµžæ•°
    likes = 0
    like_selectors = ['[class*="like"]', '[class*="count"]', '.likes']
    for sel in like_selectors:
        try:
            like_elem = elem.query_selector(sel)
            if like_elem:
                like_text = like_elem.inner_text().strip()
                # æå–æ•°å­—
                nums = re.findall(r'\d+', like_text)
                if nums:
                    likes = int(nums[0])
                    break
        except:
            pass
    
    # å‘å¸ƒæ—¶é—´
    pub_time = ""
    time_selectors = ['[class*="time"]', '[class*="date"]', 'time']
    for sel in time_selectors:
        try:
            time_elem = elem.query_selector(sel)
            if time_elem:
                pub_time = time_elem.inner_text().strip()
                if pub_time:
                    break
        except:
            pass
    
    # æ˜¯å¦ä½œè€…å›žå¤
    is_author = False
    try:
        author_badge = elem.query_selector('[class*="author"], [class*="ä½œè€…"]')
        is_author = author_badge is not None
    except:
        pass
    
    # å­è¯„è®º
    sub_comments = []
    try:
        sub_elems = elem.query_selector_all('[class*="reply"], [class*="sub-comment"]')
        for sub in sub_elems[:5]:  # æœ€å¤šå–5æ¡å­è¯„è®º
            sub_content = sub.inner_text().strip()
            if sub_content:
                sub_comments.append(sub_content[:200])
    except:
        pass
    
    return {
        "index": index,
        "nickname": nickname,
        "content": content[:500],  # é™åˆ¶é•¿åº¦
        "likes": likes,
        "time": pub_time,
        "is_author_reply": is_author,
        "sub_comments": sub_comments
    }


def main():
    parser = argparse.ArgumentParser(description="å°çº¢ä¹¦è¯„è®ºæŠ“å–å·¥å…·")
    parser.add_argument("url", help="å°çº¢ä¹¦å¸–å­é“¾æŽ¥")
    parser.add_argument("--output", "-o", default="comments.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-scroll", type=int, default=50, help="æœ€å¤§æ»šåŠ¨æ¬¡æ•°")
    parser.add_argument("--headless", action="store_true", help="æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨ï¼‰")
    
    args = parser.parse_args()
    extract_comments(args.url, args.output, args.max_scroll, args.headless)


if __name__ == "__main__":
    main()
