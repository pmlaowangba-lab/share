#!/usr/bin/env python3
"""
è¯é¢‘ä¸æƒ…æ„Ÿåˆ†æè„šæœ¬
åˆ†æè¯„è®ºé«˜é¢‘è¯ã€æƒ…æ„Ÿå€¾å‘ï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Š
"""

import argparse
import json
import re
from pathlib import Path
from collections import Counter
from datetime import datetime

def analyze_keywords(json_path: str, output_path: str, top_n: int = 50):
    """
    åˆ†æè¯„è®ºè¯é¢‘å’Œæƒ…æ„Ÿ
    
    Args:
        json_path: è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡º Excel æ–‡ä»¶è·¯å¾„
        top_n: TOP N é«˜é¢‘è¯
    """
    # æ£€æŸ¥ä¾èµ–
    try:
        import jieba
        import jieba.analyse
    except ImportError:
        print("é”™è¯¯: è¯·å…ˆå®‰è£… jieba: pip install jieba")
        return None
    
    try:
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        from openpyxl.utils import get_column_letter
    except ImportError:
        print("é”™è¯¯: è¯·å…ˆå®‰è£… openpyxl: pip install openpyxl")
        return None
    
    try:
        from snownlp import SnowNLP
        has_snownlp = True
    except ImportError:
        print("è­¦å‘Š: æœªå®‰è£… snownlpï¼Œè·³è¿‡æƒ…æ„Ÿåˆ†æ")
        has_snownlp = False
    
    # è¯»å–æ•°æ®
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    comments = data.get("comments", [])
    if not comments:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°è¯„è®ºæ•°æ®")
        return None
    
    print(f"æ­£åœ¨åˆ†æ {len(comments)} æ¡è¯„è®º...")
    
    # åˆå¹¶æ‰€æœ‰è¯„è®ºæ–‡æœ¬
    all_text = "\n".join([c.get("content", "") for c in comments])
    
    # åœç”¨è¯
    stopwords = set([
        "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "ä»¬", "è¿™", "é‚£",
        "æœ‰", "åœ¨", "ä¸", "ä¹Ÿ", "å°±", "éƒ½", "è¦", "ä¼š", "å¾ˆ", "åˆ°", "è¯´",
        "è¿˜", "èƒ½", "å¯¹", "å’Œ", "ä¸", "å—", "å§", "å•Š", "å‘¢", "å“¦", "å—¯",
        "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å“ª", "å“ªé‡Œ", "è¿™ä¸ª", "é‚£ä¸ª", "ä¸€ä¸ª",
        "å¯ä»¥", "æ²¡æœ‰", "å› ä¸º", "æ‰€ä»¥", "ä½†æ˜¯", "å¦‚æœ", "è™½ç„¶", "è€Œä¸”",
        "æˆ–è€…", "ä»¥åŠ", "æ¯”å¦‚", "å°±æ˜¯", "ä¸æ˜¯", "å¯èƒ½", "åº”è¯¥", "è§‰å¾—",
        "çŸ¥é“", "çœ‹åˆ°", "æ„Ÿè§‰", "çœŸçš„", "ç¡®å®", "å…¶å®", "ç„¶å", "å·²ç»"
    ])
    
    # åˆ†è¯ç»Ÿè®¡
    words = jieba.cut(all_text)
    word_counts = Counter()
    
    for word in words:
        word = word.strip()
        if len(word) >= 2 and word not in stopwords and not word.isdigit():
            if not re.match(r'^[\W_]+$', word):  # æ’é™¤çº¯æ ‡ç‚¹
                word_counts[word] += 1
    
    top_words = word_counts.most_common(top_n)
    
    # æå–å…³é”®è¯ï¼ˆTF-IDFï¼‰
    keywords_tfidf = jieba.analyse.extract_tags(all_text, topK=20, withWeight=True)
    
    # æƒ…æ„Ÿåˆ†æ
    sentiments = []
    if has_snownlp:
        for comment in comments:
            content = comment.get("content", "")
            if content:
                try:
                    s = SnowNLP(content)
                    score = s.sentiments  # 0-1, è¶Šå¤§è¶Šæ­£é¢
                    if score > 0.6:
                        sentiment = "æ­£é¢"
                    elif score < 0.4:
                        sentiment = "è´Ÿé¢"
                    else:
                        sentiment = "ä¸­æ€§"
                    sentiments.append({
                        "content": content[:100],
                        "score": round(score, 3),
                        "sentiment": sentiment,
                        "likes": comment.get("likes", 0)
                    })
                except:
                    pass
    
    # ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
    sentiment_counts = Counter([s["sentiment"] for s in sentiments])
    
    # æå–ç—›ç‚¹å…³é”®è¯ï¼ˆè´Ÿé¢è¯„è®ºä¸­çš„é«˜é¢‘è¯ï¼‰
    pain_words = Counter()
    if sentiments:
        negative_texts = [s["content"] for s in sentiments if s["sentiment"] == "è´Ÿé¢"]
        for text in negative_texts:
            for word in jieba.cut(text):
                word = word.strip()
                if len(word) >= 2 and word not in stopwords:
                    pain_words[word] += 1
    
    # åˆ›å»º Excel
    wb = Workbook()
    
    # ===== Sheet 1: é«˜é¢‘è¯ç»Ÿè®¡ =====
    ws1 = wb.active
    ws1.title = "é«˜é¢‘è¯TOP50"
    
    header_font = Font(bold=True, color="FFFFFF")
    header_fill = PatternFill("solid", fgColor="4472C4")
    
    headers1 = ["æ’å", "è¯è¯­", "å‡ºç°æ¬¡æ•°", "è¯é¢‘å æ¯”"]
    for col, header in enumerate(headers1, 1):
        cell = ws1.cell(row=1, column=col, value=header)
        cell.font = header_font
        cell.fill = header_fill
    
    total_words = sum(word_counts.values())
    for row, (word, count) in enumerate(top_words, 2):
        ws1.cell(row=row, column=1, value=row - 1)
        ws1.cell(row=row, column=2, value=word)
        ws1.cell(row=row, column=3, value=count)
        ws1.cell(row=row, column=4, value=f"{count/total_words*100:.2f}%")
    
    ws1.column_dimensions["B"].width = 20
    
    # ===== Sheet 2: TF-IDF å…³é”®è¯ =====
    ws2 = wb.create_sheet("TF-IDFå…³é”®è¯")
    
    headers2 = ["æ’å", "å…³é”®è¯", "æƒé‡"]
    for col, header in enumerate(headers2, 1):
        cell = ws2.cell(row=1, column=col, value=header)
        cell.font = header_font
        cell.fill = header_fill
    
    for row, (word, weight) in enumerate(keywords_tfidf, 2):
        ws2.cell(row=row, column=1, value=row - 1)
        ws2.cell(row=row, column=2, value=word)
        ws2.cell(row=row, column=3, value=round(weight, 4))
    
    ws2.column_dimensions["B"].width = 20
    
    # ===== Sheet 3: æƒ…æ„Ÿåˆ†æ =====
    if sentiments:
        ws3 = wb.create_sheet("æƒ…æ„Ÿåˆ†æ")
        
        # æ±‡æ€»
        ws3.cell(row=1, column=1, value="æƒ…æ„Ÿåˆ†å¸ƒæ±‡æ€»").font = Font(bold=True)
        ws3.cell(row=2, column=1, value="æ­£é¢")
        ws3.cell(row=2, column=2, value=sentiment_counts.get("æ­£é¢", 0))
        ws3.cell(row=3, column=1, value="ä¸­æ€§")
        ws3.cell(row=3, column=2, value=sentiment_counts.get("ä¸­æ€§", 0))
        ws3.cell(row=4, column=1, value="è´Ÿé¢")
        ws3.cell(row=4, column=2, value=sentiment_counts.get("è´Ÿé¢", 0))
        
        # æ˜ç»†
        headers3 = ["è¯„è®ºå†…å®¹", "æƒ…æ„Ÿå¾—åˆ†", "æƒ…æ„Ÿç±»å‹", "ç‚¹èµæ•°"]
        for col, header in enumerate(headers3, 1):
            cell = ws3.cell(row=6, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
        
        # æŒ‰ç‚¹èµæ•°æ’åº
        sorted_sentiments = sorted(sentiments, key=lambda x: x["likes"], reverse=True)
        for row, s in enumerate(sorted_sentiments[:100], 7):
            ws3.cell(row=row, column=1, value=s["content"])
            ws3.cell(row=row, column=2, value=s["score"])
            ws3.cell(row=row, column=3, value=s["sentiment"])
            ws3.cell(row=row, column=4, value=s["likes"])
            
            # æ ¹æ®æƒ…æ„Ÿç±»å‹ç€è‰²
            if s["sentiment"] == "è´Ÿé¢":
                ws3.cell(row=row, column=3).fill = PatternFill("solid", fgColor="FFC7CE")
            elif s["sentiment"] == "æ­£é¢":
                ws3.cell(row=row, column=3).fill = PatternFill("solid", fgColor="C6EFCE")
        
        ws3.column_dimensions["A"].width = 60
    
    # ===== Sheet 4: ç—›ç‚¹å…³é”®è¯ =====
    if pain_words:
        ws4 = wb.create_sheet("ç—›ç‚¹å…³é”®è¯")
        
        headers4 = ["æ’å", "ç—›ç‚¹è¯", "å‡ºç°æ¬¡æ•°"]
        for col, header in enumerate(headers4, 1):
            cell = ws4.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
        
        for row, (word, count) in enumerate(pain_words.most_common(30), 2):
            ws4.cell(row=row, column=1, value=row - 1)
            ws4.cell(row=row, column=2, value=word)
            ws4.cell(row=row, column=3, value=count)
        
        ws4.column_dimensions["B"].width = 20
    
    # ä¿å­˜
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    wb.save(output_file)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“„ ä¿å­˜è‡³: {output_file}")
    print(f"\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
    print(f"   - é«˜é¢‘è¯ TOP5: {', '.join([w for w, c in top_words[:5]])}")
    print(f"   - TF-IDF å…³é”®è¯: {', '.join([w for w, _ in keywords_tfidf[:5]])}")
    if sentiments:
        print(f"   - æƒ…æ„Ÿåˆ†å¸ƒ: æ­£é¢ {sentiment_counts.get('æ­£é¢', 0)}, ä¸­æ€§ {sentiment_counts.get('ä¸­æ€§', 0)}, è´Ÿé¢ {sentiment_counts.get('è´Ÿé¢', 0)}")
    if pain_words:
        print(f"   - ç—›ç‚¹è¯ TOP5: {', '.join([w for w, c in pain_words.most_common(5)])}")
    
    return {
        "top_words": top_words,
        "keywords_tfidf": keywords_tfidf,
        "sentiment_counts": dict(sentiment_counts),
        "pain_words": pain_words.most_common(30)
    }


def main():
    parser = argparse.ArgumentParser(description="è¯„è®ºè¯é¢‘ä¸æƒ…æ„Ÿåˆ†æ")
    parser.add_argument("json_file", help="è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", default="åˆ†æç»“æœ.xlsx", help="è¾“å‡º Excel æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--top", type=int, default=50, help="TOP N é«˜é¢‘è¯")
    
    args = parser.parse_args()
    analyze_keywords(args.json_file, args.output, args.top)


if __name__ == "__main__":
    main()
